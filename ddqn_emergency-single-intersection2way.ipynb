{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env SUMO_HOME=\"/Library/Frameworks/EclipseSUMO.framework/Versions/1.22.0/EclipseSUMO/share/sumo\"\n",
    "# %env LIBSUMO_AS_TRACI=1\n",
    "# %env PATH=$PATH:/Library/Frameworks/EclipseSUMO.framework/Versions/1.22.0/EclipseSUMO/share/sumo/bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "\n",
    "from env import SumoEnvironment\n",
    "from sumo_rl.agents import QLAgent\n",
    "from sumo_rl.exploration import EpsilonGreedy\n",
    "\n",
    "from custom_observation import CustomEmergencyObservationFunction\n",
    "from custom_reward import emergency_reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, inp_size, op_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.hl1 = nn.Linear(inp_size, 256)\n",
    "        self.hl2 = nn.Linear(256, 128)\n",
    "        self.fc = nn.Linear(128, op_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_normal_(self.hl1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.hl2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.hl1.bias)\n",
    "        torch.nn.init.zeros_(self.hl2.bias)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hl1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hl2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent:\n",
    "    \n",
    "    def __init__(self, model, env, initial_epsilon, epsilon_decay, final_epsilon, discount_factor, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "    def step(self, obs, train=True):\n",
    "        if train:\n",
    "            if np.random.random() < self.epsilon:\n",
    "                return self.action_space.sample()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(torch.from_numpy(obs['t']).to(dtype=torch.float32))\n",
    "                    return torch.argmax(output).item()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.model(torch.from_numpy(obs['t']).to(dtype=torch.float32))\n",
    "                return torch.argmax(output).item()\n",
    "        \n",
    "\n",
    "    def decay(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae6952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        use_gui=False,\n",
    "        num_seconds=250,\n",
    "        enforce_max_green=False,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "BUFFER_SIZE = 1200\n",
    "batch_size = 32\n",
    "sync_target_step = 300\n",
    "train_every_step = 1\n",
    "episodes = 10\n",
    "replay_buffer = torch.zeros((BUFFER_SIZE, 32 + 1 + 1 + 32 + 1), dtype = torch.float32) #[sx9, a, r, s'x9, terminate flag]\n",
    "learning_rate = 0.01\n",
    "\n",
    "#agent params\n",
    "start_epsilon = 1\n",
    "final_epsilon = 0.01\n",
    "epsilon_decay = (final_epsilon/start_epsilon)**(1/episodes)  # exponential decay\n",
    "discount_factor = 0.99\n",
    "\n",
    "# model init\n",
    "# print(env.observation_space.shape[0])\n",
    "# print(env.action_space.n)\n",
    "model = DQNetwork(32, env.action_space.n)\n",
    "target_model = deepcopy(model)\n",
    "# m = torch.load(\"emergency_dqn.pth\")\n",
    "# model.load_state_dict(m)\n",
    "agent = GreedyAgent(model, env, start_epsilon, epsilon_decay, final_epsilon, discount_factor)\n",
    "\n",
    "# optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 0 \n",
    "train_loss = []\n",
    "reward_per_episode = [0] * episodes\n",
    "epsilon_per_episode = [0] * episodes\n",
    "steps_per_episode = [0] * episodes\n",
    "wait_per_episode_lane_1 = [0] * episodes\n",
    "wait_per_episode_lane_2 = [0] * episodes\n",
    "wait_per_episode_lane_3 = [0] * episodes\n",
    "wait_per_episode_lane_4 = [0] * episodes\n",
    "wait_per_episode_lane_5 = [0] * episodes\n",
    "wait_per_episode_lane_6 = [0] * episodes\n",
    "wait_per_episode_lane_7 = [0] * episodes\n",
    "wait_per_episode_lane_8 = [0] * episodes\n",
    "wait_per_episode_lane_9 = [0] * episodes\n",
    "wait_per_episode_lane_10 = [0] * episodes\n",
    "wait_per_episode_lane_11 = [0] * episodes\n",
    "wait_per_episode_lane_12 = [0] * episodes\n",
    "\n",
    "for episode in range(episodes):\n",
    "    print(\"Episode: \", episode)\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "    epsilon_per_episode[episode] = agent.epsilon\n",
    "    batch_loss = []\n",
    "    t = 0\n",
    "    lane_1_wt, lane_2_wt = 0, 0\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs)\n",
    "        next_obs, reward, done, info = env.step({'t': action})\n",
    "        \n",
    "        # print(\":\", len(info[\"emergency_waiting_time\"]))\n",
    "        if len(info[\"emergency_waiting_time\"]) > 0:\n",
    "            for k,v in info[\"emergency_waiting_time\"].items():\n",
    "                if k == \"flow_emergency_ns\":\n",
    "                    wait_per_episode_lane_1[episode] += v\n",
    "                elif k == \"flow_emergency_nw\":\n",
    "                    wait_per_episode_lane_2[episode] += v\n",
    "                elif k == \"flow_emergency_ne\":\n",
    "                    wait_per_episode_lane_3[episode] += v\n",
    "                elif k == \"flow_emergency_we\":\n",
    "                    wait_per_episode_lane_4[episode] += v\n",
    "                elif k == \"flow_emergency_wn\":\n",
    "                    wait_per_episode_lane_5[episode] += v\n",
    "                elif k == \"flow_emergency_ws\":\n",
    "                    wait_per_episode_lane_6[episode] += v\n",
    "                elif k == \"flow_emergency_ew\":\n",
    "                    wait_per_episode_lane_7[episode] += v\n",
    "                elif k == \"flow_emergency_en\":\n",
    "                    wait_per_episode_lane_8[episode] += v\n",
    "                elif k == \"flow_emergency_es\":\n",
    "                    wait_per_episode_lane_9[episode] += v\n",
    "                elif k == \"flow_emergency_sn\":\n",
    "                    wait_per_episode_lane_10[episode] += v\n",
    "                elif k == \"flow_emergency_se\":\n",
    "                    wait_per_episode_lane_11[episode] += v\n",
    "                elif k == \"flow_emergency_sw\":\n",
    "                    wait_per_episode_lane_12[episode] += v\n",
    "        \n",
    "        n_iters = n_iters + 1\n",
    "        \n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        replay_buffer[n_iters % BUFFER_SIZE] = torch.cat([torch.from_numpy(obs['t']), torch.tensor([action], dtype=torch.int32), torch.tensor([reward['t']], dtype=torch.int32), torch.from_numpy(next_obs['t']), torch.tensor([int(done[\"__all__\"])], dtype=torch.int32)]).to(torch.float32)\n",
    "        # print((next_obs['t'].shape))\n",
    "        obs = deepcopy(next_obs)\n",
    "        t += 1\n",
    "    \n",
    "    steps_per_episode[episode] += t\n",
    "    agent.decay()\n",
    "    env.close()\n",
    "    \n",
    "    if n_iters >= batch_size and episode % train_every_step == 0:\n",
    "\n",
    "        selected_transition_indices = torch.randint(low=0, high=min(n_iters, BUFFER_SIZE), size=(batch_size, ))\n",
    "        selected_transition = replay_buffer[selected_transition_indices]  #selected_transition: [sx24`, a, r, s'x24, terminate flag]\n",
    "\n",
    "        td_target = torch.zeros(batch_size)\n",
    "        next_obs = selected_transition[:, 34:-1]\n",
    "        model.eval()\n",
    "        target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            next_q = model(next_obs)\n",
    "            best_next_q = torch.argmax(next_q, dim = 1)\n",
    "            target_output = target_model(next_obs)\n",
    "            q_values = target_output.gather(1, best_next_q.unsqueeze(1)).squeeze(1)\n",
    "            td_target = torch.where(selected_transition[:, -1] == True, selected_transition[:, 33], selected_transition[:, 33] + discount_factor * q_values)\n",
    "\n",
    "        model.train()\n",
    "        obs = selected_transition[:, :32]\n",
    "        output = model(obs)\n",
    "        td_estimate = output.gather(1, selected_transition[:, 32][:, None].type(torch.int64)).squeeze(1)\n",
    "        loss = criterion(td_target, td_estimate)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if(episode % sync_target_step == 0):\n",
    "        target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(8, 2, figsize=(20, 15))\n",
    "\n",
    "axs[0, 0].plot(range(episodes), steps_per_episode)\n",
    "axs[0, 0].set_title(\"steps per episode \")\n",
    "axs[0, 0].set_xlabel(\"Episode\")\n",
    "axs[0, 0].set_ylabel(\"Steps\")\n",
    "\n",
    "axs[0, 1].plot(range(episodes), reward_per_episode)\n",
    "axs[0, 1].set_title(\"total reward per episode \")\n",
    "axs[0, 1].set_xlabel(\"Episode\")\n",
    "axs[0, 1].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1, 0].plot(range(episodes), epsilon_per_episode)\n",
    "axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "axs[1, 0].set_xlabel(\"Episode\")\n",
    "axs[1, 0].set_ylabel(\"Epsilon Value\")\n",
    "\n",
    "axs[1, 1].plot(range(len(train_loss)), train_loss)\n",
    "axs[1, 1].set_title(\"Training Loss\")\n",
    "axs[1, 1].set_xlabel(\"Episode\")\n",
    "axs[1, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[2, 0].plot(range(len(wait_per_episode_lane_1)), wait_per_episode_lane_1)\n",
    "axs[2, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[2, 0].set_xlabel(\"Episode\")\n",
    "axs[2, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[2, 1].plot(range(len(wait_per_episode_lane_2)), wait_per_episode_lane_2)\n",
    "axs[2, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[2, 1].set_xlabel(\"Episode\")\n",
    "axs[2, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[3, 0].plot(range(len(wait_per_episode_lane_3)), wait_per_episode_lane_3)\n",
    "axs[3, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[3, 0].set_xlabel(\"Episode\")\n",
    "axs[3, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[3, 1].plot(range(len(wait_per_episode_lane_4)), wait_per_episode_lane_4)\n",
    "axs[3, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[3, 1].set_xlabel(\"Episode\")\n",
    "axs[3, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[4, 0].plot(range(len(wait_per_episode_lane_5)), wait_per_episode_lane_5)\n",
    "axs[4, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[4, 0].set_xlabel(\"Episode\")\n",
    "axs[4, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[4, 1].plot(range(len(wait_per_episode_lane_6)), wait_per_episode_lane_6)\n",
    "axs[4, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[4, 1].set_xlabel(\"Episode\")\n",
    "axs[4, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[5, 0].plot(range(len(wait_per_episode_lane_7)), wait_per_episode_lane_7)\n",
    "axs[5, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[5, 0].set_xlabel(\"Episode\")\n",
    "axs[5, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[5, 1].plot(range(len(wait_per_episode_lane_8)), wait_per_episode_lane_8)\n",
    "axs[5, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[5, 1].set_xlabel(\"Episode\")\n",
    "axs[5, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[6, 0].plot(range(len(wait_per_episode_lane_9)), wait_per_episode_lane_9)\n",
    "axs[6, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[6, 0].set_xlabel(\"Episode\")\n",
    "axs[6, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[6, 1].plot(range(len(wait_per_episode_lane_10)), wait_per_episode_lane_10)\n",
    "axs[6, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[6, 1].set_xlabel(\"Episode\")\n",
    "axs[6, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[7, 0].plot(range(len(wait_per_episode_lane_11)), wait_per_episode_lane_11)\n",
    "axs[7, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[7, 0].set_xlabel(\"Episode\")\n",
    "axs[7, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[7, 1].plot(range(len(wait_per_episode_lane_12)), wait_per_episode_lane_12)\n",
    "axs[7, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[7, 1].set_xlabel(\"Episode\")\n",
    "axs[7, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=5000,\n",
    "        enforce_max_green=False,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )\n",
    "n_episodes = 5\n",
    "agent.epsilon = 0\n",
    "reward_per_episode = [0]*n_episodes\n",
    "wait_per_episode_lane_1 = [0] * n_episodes\n",
    "wait_per_episode_lane_2 = [0] * n_episodes\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "\n",
    "    # play one episode\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs, train=False)\n",
    "        next_obs, reward, done, info = env.step({'t': action})\n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        obs = deepcopy(next_obs)\n",
    "\n",
    "        if len(info[\"emergency_waiting_time\"]) > 0:\n",
    "            for k,v in info[\"emergency_waiting_time\"].items():\n",
    "                if k == \"flow_emergency_ns_1\":\n",
    "                    wait_per_episode_lane_1[episode] += v\n",
    "                elif k == \"flow_emergency_ns_2\":\n",
    "                    wait_per_episode_lane_2[episode] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
    "\n",
    "axs[0].plot(range(n_episodes), reward_per_episode)\n",
    "axs[0].set_title(\"total reward per episode \")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1].plot(range(len(wait_per_episode_lane_1)), wait_per_episode_lane_1)\n",
    "axs[1].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[2].plot(range(len(wait_per_episode_lane_2)), wait_per_episode_lane_2)\n",
    "axs[2].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93c4e0-92d9-4755-a16e-707a57124cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"emergency_ddqn2way.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba44a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traci\n",
    "\n",
    "if traci.isLoaded():\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816383b-b46d-4303-9cf9-36a58e819843",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=5000,\n",
    "        # reward_fn=[\"diff-waiting-time\", \"average-speed\"],\n",
    "        # reward_weights=[1, 0.1],\n",
    "        enforce_max_green=True,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )\n",
    "\n",
    "n_episodes = 1\n",
    "agent.epsilon = 0\n",
    "reward_per_episode = [0]*n_episodes\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "\n",
    "    # play one episode\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs, train=False)\n",
    "        next_obs, reward, done, _ = env.step({'t': action})\n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        obs = deepcopy(next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883abe8-5909-44fd-a6eb-aabbb2d7745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = SumoEnvironment(\n",
    "#         net_file=\"single-intersection/single-intersection.net.xml\",\n",
    "#         route_file=\"single-intersection/single-intersection.rou.xml\",\n",
    "#         out_csv_name=\"out_csv\",\n",
    "#         use_gui=True,\n",
    "#         num_seconds=500,\n",
    "#         min_green=5,\n",
    "#         max_green=10,\n",
    "#         observation_class = CustomEmergencyObservationFunction,\n",
    "#         reward_fn = emergency_reward_fn\n",
    "#     )\n",
    "\n",
    "# n_episodes = 1\n",
    "# for episode in range(n_episodes):\n",
    "#     obs = env.reset()\n",
    "#     done = {\"__all__\": False}\n",
    "#     while not done[\"__all__\"]:\n",
    "#         next_obs, reward, done, _ = env.step({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d7618-1b51-458a-a456-f33f8a7ecd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=5000,\n",
    "        # reward_fn=[\"diff-waiting-time\", \"average-speed\"],\n",
    "        # reward_weights=[1, 0.1],\n",
    "        enforce_max_green=True,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )\n",
    "episodes = 1\n",
    "agent.epsilon = 0\n",
    "reward_per_episode = [0]*n_episodes\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "\n",
    "    # play one episode\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs, train=False)\n",
    "        next_obs, reward, done, _ = env.step({'t': action})\n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        obs = deepcopy(next_obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
