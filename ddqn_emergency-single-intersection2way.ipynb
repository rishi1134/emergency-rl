{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d165cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env SUMO_HOME=\"/Library/Frameworks/EclipseSUMO.framework/Versions/1.22.0/EclipseSUMO/share/sumo\"\n",
    "# %env LIBSUMO_AS_TRACI=1\n",
    "# %env PATH=$PATH:/Library/Frameworks/EclipseSUMO.framework/Versions/1.22.0/EclipseSUMO/share/sumo/bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a15c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"SUMO_HOME\" in os.environ:\n",
    "    tools = os.path.join(os.environ[\"SUMO_HOME\"], \"tools\")\n",
    "    sys.path.append(tools)\n",
    "else:\n",
    "    sys.exit(\"Please declare the environment variable 'SUMO_HOME'\")\n",
    "\n",
    "from env import SumoEnvironment\n",
    "from sumo_rl.agents import QLAgent\n",
    "from sumo_rl.exploration import EpsilonGreedy\n",
    "\n",
    "from custom_observation import CustomEmergencyObservationFunction\n",
    "from custom_reward import emergency_reward_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380bf392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, inp_size, op_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.hl1 = nn.Linear(inp_size, 256)\n",
    "        self.hl2 = nn.Linear(256, 128)\n",
    "        self.fc = nn.Linear(128, op_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_normal_(self.hl1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.hl2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.hl1.bias)\n",
    "        torch.nn.init.zeros_(self.hl2.bias)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hl1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hl2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367620a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAgent:\n",
    "    \n",
    "    def __init__(self, model, env, initial_epsilon, epsilon_decay, final_epsilon, discount_factor, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "    def step(self, obs, train=True):\n",
    "        if train:\n",
    "            if np.random.random() < self.epsilon:\n",
    "                return self.action_space.sample()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(torch.from_numpy(obs['t']).to(dtype=torch.float32))\n",
    "                    return torch.argmax(output).item()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = self.model(torch.from_numpy(obs['t']).to(dtype=torch.float32))\n",
    "                return torch.argmax(output).item()\n",
    "        \n",
    "\n",
    "    def decay(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae6952a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrying in 1 seconds\n",
      "Step #0.00 (0ms ?*RT. ?UPS, TraCI: 7ms, vehicles TOT 0 ACT 0 BUF 0)                      \n"
     ]
    }
   ],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        route_file=\"single_2way/single-intersection-horizontal.rou.xml\",\n",
    "        # use_gui=False,\n",
    "        use_gui=True,\n",
    "        num_seconds=1250,\n",
    "        enforce_max_green=False,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090cb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "BUFFER_SIZE = 10000\n",
    "batch_size = 32\n",
    "sync_target_step = 300\n",
    "train_every_step = 1\n",
    "episodes = 300\n",
    "replay_buffer = torch.zeros((BUFFER_SIZE, 32 + 1 + 1 + 32 + 1), dtype = torch.float32) #[sx9, a, r, s'x9, terminate flag]\n",
    "learning_rate = 0.01\n",
    "\n",
    "#agent params\n",
    "start_epsilon = 1\n",
    "final_epsilon = 0.01\n",
    "epsilon_decay = (final_epsilon/start_epsilon)**(1/episodes)  # exponential decay\n",
    "discount_factor = 0.99\n",
    "\n",
    "# model init\n",
    "# print(env.observation_space.shape[0])\n",
    "# print(env.action_space.n)\n",
    "model = DQNetwork(32, env.action_space.n)\n",
    "target_model = deepcopy(model)\n",
    "# m = torch.load(\"emergency_dqn.pth\")\n",
    "# model.load_state_dict(m)\n",
    "agent = GreedyAgent(model, env, start_epsilon, epsilon_decay, final_epsilon, discount_factor)\n",
    "\n",
    "# optimizer and criterion\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf1d003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/v43sm45x2sv6hy0lmm9hd4f40000gn/T/ipykernel_22086/3044515802.py:62: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  replay_buffer[n_iters % BUFFER_SIZE] = torch.cat([torch.from_numpy(obs['t']), torch.tensor([action], dtype=torch.int32), torch.tensor([reward['t']], dtype=torch.int32), torch.from_numpy(next_obs['t']), torch.tensor([int(done[\"__all__\"])], dtype=torch.int32)]).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1\n",
      " Retrying in 1 seconds\n",
      "Episode:  2\n",
      " Retrying in 1 seconds\n",
      "Episode:  3\n",
      " Retrying in 1 seconds\n",
      "Episode:  4\n",
      " Retrying in 1 seconds\n"
     ]
    },
    {
     "ename": "FatalTraCIError",
     "evalue": "Connection closed by SUMO.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     28\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mstep(obs)\n\u001b[0;32m---> 29\u001b[0m     next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# print(\":\", len(info[\"emergency_waiting_time\"]))\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memergency_waiting_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/CSE/RL/rl-final/env.py:286\u001b[0m, in \u001b[0;36mSumoEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_actions(action)\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_observations()\n\u001b[1;32m    289\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_rewards()\n",
      "File \u001b[0;32m~/Documents/CSE/RL/rl-final/env.py:303\u001b[0m, in \u001b[0;36mSumoEnvironment._run_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m time_to_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m time_to_act:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sumo_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mts_ids:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraffic_signals[ts]\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/Documents/CSE/RL/rl-final/env.py:403\u001b[0m, in \u001b[0;36mSumoEnvironment._sumo_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sumo_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_arrived_vehicles \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39msimulation\u001b[38;5;241m.\u001b[39mgetArrivedNumber()\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_departed_vehicles \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39msimulation\u001b[38;5;241m.\u001b[39mgetDepartedNumber()\n",
      "File \u001b[0;32m~/Documents/CSE/RL/rl-final/ve/lib/python3.9/site-packages/traci/connection.py:370\u001b[0m, in \u001b[0;36mConnection.simulationStep\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m    369\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI change now handles step as floating point seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 370\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptionMapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    372\u001b[0m     subscriptionResults\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m~/Documents/CSE/RL/rl-final/ve/lib/python3.9/site-packages/traci/connection.py:232\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[0;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CSE/RL/rl-final/ve/lib/python3.9/site-packages/traci/connection.py:137\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection closed by SUMO.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m command \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue:\n\u001b[1;32m    139\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BBB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFatalTraCIError\u001b[0m: Connection closed by SUMO."
     ]
    }
   ],
   "source": [
    "n_iters = 0 \n",
    "train_loss = []\n",
    "reward_per_episode = [0] * episodes\n",
    "epsilon_per_episode = [0] * episodes\n",
    "steps_per_episode = [0] * episodes\n",
    "wait_per_episode_lane_1 = [0] * episodes\n",
    "wait_per_episode_lane_2 = [0] * episodes\n",
    "wait_per_episode_lane_3 = [0] * episodes\n",
    "wait_per_episode_lane_4 = [0] * episodes\n",
    "wait_per_episode_lane_5 = [0] * episodes\n",
    "wait_per_episode_lane_6 = [0] * episodes\n",
    "wait_per_episode_lane_7 = [0] * episodes\n",
    "wait_per_episode_lane_8 = [0] * episodes\n",
    "wait_per_episode_lane_9 = [0] * episodes\n",
    "wait_per_episode_lane_10 = [0] * episodes\n",
    "wait_per_episode_lane_11 = [0] * episodes\n",
    "wait_per_episode_lane_12 = [0] * episodes\n",
    "\n",
    "for episode in range(episodes):\n",
    "    print(\"Episode: \", episode)\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "    epsilon_per_episode[episode] = agent.epsilon\n",
    "    batch_loss = []\n",
    "    t = 0\n",
    "    lane_1_wt, lane_2_wt = 0, 0\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs)\n",
    "        next_obs, reward, done, info = env.step({'t': action})\n",
    "        \n",
    "        # print(\":\", len(info[\"emergency_waiting_time\"]))\n",
    "        if len(info[\"emergency_waiting_time\"]) > 0:\n",
    "            for k,v in info[\"emergency_waiting_time\"].items():\n",
    "                if k == \"flow_emergency_ns\":\n",
    "                    wait_per_episode_lane_1[episode] += v\n",
    "                elif k == \"flow_emergency_nw\":\n",
    "                    wait_per_episode_lane_2[episode] += v\n",
    "                elif k == \"flow_emergency_ne\":\n",
    "                    wait_per_episode_lane_3[episode] += v\n",
    "                elif k == \"flow_emergency_we\":\n",
    "                    wait_per_episode_lane_4[episode] += v\n",
    "                elif k == \"flow_emergency_wn\":\n",
    "                    wait_per_episode_lane_5[episode] += v\n",
    "                elif k == \"flow_emergency_ws\":\n",
    "                    wait_per_episode_lane_6[episode] += v\n",
    "                elif k == \"flow_emergency_ew\":\n",
    "                    wait_per_episode_lane_7[episode] += v\n",
    "                elif k == \"flow_emergency_en\":\n",
    "                    wait_per_episode_lane_8[episode] += v\n",
    "                elif k == \"flow_emergency_es\":\n",
    "                    wait_per_episode_lane_9[episode] += v\n",
    "                elif k == \"flow_emergency_sn\":\n",
    "                    wait_per_episode_lane_10[episode] += v\n",
    "                elif k == \"flow_emergency_se\":\n",
    "                    wait_per_episode_lane_11[episode] += v\n",
    "                elif k == \"flow_emergency_sw\":\n",
    "                    wait_per_episode_lane_12[episode] += v\n",
    "        \n",
    "        n_iters = n_iters + 1\n",
    "        \n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        replay_buffer[n_iters % BUFFER_SIZE] = torch.cat([torch.from_numpy(obs['t']), torch.tensor([action], dtype=torch.int32), torch.tensor([reward['t']], dtype=torch.int32), torch.from_numpy(next_obs['t']), torch.tensor([int(done[\"__all__\"])], dtype=torch.int32)]).to(torch.float32)\n",
    "        # print((next_obs['t'].shape))\n",
    "        obs = deepcopy(next_obs)\n",
    "        t += 1\n",
    "    \n",
    "    steps_per_episode[episode] += t\n",
    "    agent.decay()\n",
    "    env.close()\n",
    "    \n",
    "    if n_iters >= batch_size and episode % train_every_step == 0:\n",
    "\n",
    "        selected_transition_indices = torch.randint(low=0, high=min(n_iters, BUFFER_SIZE), size=(batch_size, ))\n",
    "        selected_transition = replay_buffer[selected_transition_indices]  #selected_transition: [sx24`, a, r, s'x24, terminate flag]\n",
    "\n",
    "        td_target = torch.zeros(batch_size)\n",
    "        next_obs = selected_transition[:, 34:-1]\n",
    "        model.eval()\n",
    "        target_model.eval()\n",
    "        with torch.no_grad():\n",
    "            next_q = model(next_obs)\n",
    "            best_next_q = torch.argmax(next_q, dim = 1)\n",
    "            target_output = target_model(next_obs)\n",
    "            q_values = target_output.gather(1, best_next_q.unsqueeze(1)).squeeze(1)\n",
    "            td_target = torch.where(selected_transition[:, -1] == True, selected_transition[:, 33], selected_transition[:, 33] + discount_factor * q_values)\n",
    "\n",
    "        model.train()\n",
    "        obs = selected_transition[:, :32]\n",
    "        output = model(obs)\n",
    "        td_estimate = output.gather(1, selected_transition[:, 32][:, None].type(torch.int64)).squeeze(1)\n",
    "        loss = criterion(td_target, td_estimate)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if(episode % sync_target_step == 0):\n",
    "        target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2269dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(8, 2, figsize=(20, 15))\n",
    "\n",
    "axs[0, 0].plot(range(episodes), steps_per_episode)\n",
    "axs[0, 0].set_title(\"steps per episode \")\n",
    "axs[0, 0].set_xlabel(\"Episode\")\n",
    "axs[0, 0].set_ylabel(\"Steps\")\n",
    "\n",
    "axs[0, 1].plot(range(episodes), reward_per_episode)\n",
    "axs[0, 1].set_title(\"total reward per episode \")\n",
    "axs[0, 1].set_xlabel(\"Episode\")\n",
    "axs[0, 1].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1, 0].plot(range(episodes), epsilon_per_episode)\n",
    "axs[1, 0].set_title(\"Epsilon Decay\")\n",
    "axs[1, 0].set_xlabel(\"Episode\")\n",
    "axs[1, 0].set_ylabel(\"Epsilon Value\")\n",
    "\n",
    "axs[1, 1].plot(range(len(train_loss)), train_loss)\n",
    "axs[1, 1].set_title(\"Training Loss\")\n",
    "axs[1, 1].set_xlabel(\"Episode\")\n",
    "axs[1, 1].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[2, 0].plot(range(len(wait_per_episode_lane_1)), wait_per_episode_lane_1)\n",
    "axs[2, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[2, 0].set_xlabel(\"Episode\")\n",
    "axs[2, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[2, 1].plot(range(len(wait_per_episode_lane_2)), wait_per_episode_lane_2)\n",
    "axs[2, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[2, 1].set_xlabel(\"Episode\")\n",
    "axs[2, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[3, 0].plot(range(len(wait_per_episode_lane_3)), wait_per_episode_lane_3)\n",
    "axs[3, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[3, 0].set_xlabel(\"Episode\")\n",
    "axs[3, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[3, 1].plot(range(len(wait_per_episode_lane_4)), wait_per_episode_lane_4)\n",
    "axs[3, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[3, 1].set_xlabel(\"Episode\")\n",
    "axs[3, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[4, 0].plot(range(len(wait_per_episode_lane_5)), wait_per_episode_lane_5)\n",
    "axs[4, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[4, 0].set_xlabel(\"Episode\")\n",
    "axs[4, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[4, 1].plot(range(len(wait_per_episode_lane_6)), wait_per_episode_lane_6)\n",
    "axs[4, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[4, 1].set_xlabel(\"Episode\")\n",
    "axs[4, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[5, 0].plot(range(len(wait_per_episode_lane_7)), wait_per_episode_lane_7)\n",
    "axs[5, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[5, 0].set_xlabel(\"Episode\")\n",
    "axs[5, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[5, 1].plot(range(len(wait_per_episode_lane_8)), wait_per_episode_lane_8)\n",
    "axs[5, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[5, 1].set_xlabel(\"Episode\")\n",
    "axs[5, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[6, 0].plot(range(len(wait_per_episode_lane_9)), wait_per_episode_lane_9)\n",
    "axs[6, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[6, 0].set_xlabel(\"Episode\")\n",
    "axs[6, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[6, 1].plot(range(len(wait_per_episode_lane_10)), wait_per_episode_lane_10)\n",
    "axs[6, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[6, 1].set_xlabel(\"Episode\")\n",
    "axs[6, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[7, 0].plot(range(len(wait_per_episode_lane_11)), wait_per_episode_lane_11)\n",
    "axs[7, 0].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[7, 0].set_xlabel(\"Episode\")\n",
    "axs[7, 0].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[7, 1].plot(range(len(wait_per_episode_lane_12)), wait_per_episode_lane_12)\n",
    "axs[7, 1].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[7, 1].set_xlabel(\"Episode\")\n",
    "axs[7, 1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        # route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        route_file=\"single_2way/single-intersection-horizontal.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=5000,\n",
    "        enforce_max_green=False,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )\n",
    "n_episodes = 5\n",
    "agent.epsilon = 0\n",
    "reward_per_episode = [0]*n_episodes\n",
    "wait_per_episode_lane_1 = [0] * n_episodes\n",
    "wait_per_episode_lane_2 = [0] * n_episodes\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "\n",
    "    # play one episode\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs, train=False)\n",
    "        next_obs, reward, done, info = env.step({'t': action})\n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        obs = deepcopy(next_obs)\n",
    "\n",
    "        if len(info[\"emergency_waiting_time\"]) > 0:\n",
    "            for k,v in info[\"emergency_waiting_time\"].items():\n",
    "                if k == \"flow_emergency_ns_1\":\n",
    "                    wait_per_episode_lane_1[episode] += v\n",
    "                elif k == \"flow_emergency_ns_2\":\n",
    "                    wait_per_episode_lane_2[episode] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# visualize the episode rewards, episode length and training error in one figure\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
    "\n",
    "axs[0].plot(range(n_episodes), reward_per_episode)\n",
    "axs[0].set_title(\"total reward per episode \")\n",
    "axs[0].set_xlabel(\"Episode\")\n",
    "axs[0].set_ylabel(\"Reward\")\n",
    "\n",
    "axs[1].plot(range(len(wait_per_episode_lane_1)), wait_per_episode_lane_1)\n",
    "axs[1].set_title(\"Lane 1 Wait Time for Emergency Vehicle\")\n",
    "axs[1].set_xlabel(\"Episode\")\n",
    "axs[1].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "axs[2].plot(range(len(wait_per_episode_lane_2)), wait_per_episode_lane_2)\n",
    "axs[2].set_title(\"Lane 2 Wait Time for Emergency Vehicle\")\n",
    "axs[2].set_xlabel(\"Episode\")\n",
    "axs[2].set_ylabel(\"Cumm. Wait time\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93c4e0-92d9-4755-a16e-707a57124cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"emergency_ddqn2way.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba44a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traci\n",
    "\n",
    "if traci.isLoaded():\n",
    "    traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816383b-b46d-4303-9cf9-36a58e819843",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        # route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        route_file=\"single_2way/single-intersection-horizontal.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=5000,\n",
    "        # reward_fn=[\"diff-waiting-time\", \"average-speed\"],\n",
    "        # reward_weights=[1, 0.1],\n",
    "        enforce_max_green=True,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )\n",
    "\n",
    "n_episodes = 1\n",
    "agent.epsilon = 0\n",
    "reward_per_episode = [0]*n_episodes\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "\n",
    "    # play one episode\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs, train=False)\n",
    "        next_obs, reward, done, _ = env.step({'t': action})\n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        obs = deepcopy(next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883abe8-5909-44fd-a6eb-aabbb2d7745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = SumoEnvironment(\n",
    "#         net_file=\"single-intersection/single-intersection.net.xml\",\n",
    "#         route_file=\"single-intersection/single-intersection.rou.xml\",\n",
    "#         out_csv_name=\"out_csv\",\n",
    "#         use_gui=True,\n",
    "#         num_seconds=500,\n",
    "#         min_green=5,\n",
    "#         max_green=10,\n",
    "#         observation_class = CustomEmergencyObservationFunction,\n",
    "#         reward_fn = emergency_reward_fn\n",
    "#     )\n",
    "\n",
    "# n_episodes = 1\n",
    "# for episode in range(n_episodes):\n",
    "#     obs = env.reset()\n",
    "#     done = {\"__all__\": False}\n",
    "#     while not done[\"__all__\"]:\n",
    "#         next_obs, reward, done, _ = env.step({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d7618-1b51-458a-a456-f33f8a7ecd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SumoEnvironment(\n",
    "        net_file=\"single_2way/single-intersection.net.xml\",\n",
    "        # route_file=\"single_2way/single-intersection-gen.rou.xml\",\n",
    "        route_file=\"single_2way/single-intersection-horizontal.rou.xml\",\n",
    "        use_gui=True,\n",
    "        num_seconds=5000,\n",
    "        # reward_fn=[\"diff-waiting-time\", \"average-speed\"],\n",
    "        # reward_weights=[1, 0.1],\n",
    "        enforce_max_green=True,\n",
    "        min_green=5,\n",
    "        delta_time=5,\n",
    "        observation_class = CustomEmergencyObservationFunction,\n",
    "        reward_fn = emergency_reward_fn\n",
    "    )\n",
    "episodes = 1\n",
    "agent.epsilon = 0\n",
    "reward_per_episode = [0]*n_episodes\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = {\"__all__\": False}\n",
    "\n",
    "    # play one episode\n",
    "    while not done[\"__all__\"]:\n",
    "        action = agent.step(obs, train=False)\n",
    "        next_obs, reward, done, _ = env.step({'t': action})\n",
    "        reward_per_episode[episode] += reward['t']\n",
    "        obs = deepcopy(next_obs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
